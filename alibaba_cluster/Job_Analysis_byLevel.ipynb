{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aybJzPV-JVCa",
        "outputId": "ba35adf3-8028-414a-d3e5-04c82ecd77d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Instructions**\n",
        "\n",
        "This code analyzes the Alibaba Clusters (visited and processed locally) by levels\n",
        "\n",
        "**Input**: 'batch_instance.csv' and 'batch_task.csv' from https://github.com/alibaba/clusterdata/blob/master/cluster-trace-v2018/fetchData.sh\n",
        "\n",
        "**Output 1**: 'level_graphene_result.txt' and 'resource_utilization_by_job.txt' contains four Graphene parameters for all of the DAGs.\n",
        "\n",
        "**Output 2**: 'levels/' directory contains all of the DAG in each level\n",
        "          'instance_level.csv' csv file temporarily contains all of the instance infomation of the all of the DAGs in the specific level"
      ],
      "metadata": {
        "id": "-haHd6Xx98xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_job_ids(file_path):\n",
        "    \"\"\"\n",
        "    Reads integers from a text file (one per line) and returns them as a list.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the text file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of integers loaded from the file.\n",
        "    \"\"\"\n",
        "    desired_levels = []\n",
        "\n",
        "    # Read the file and populate desired_levels\n",
        "    with open(file_path, \"r\") as file:\n",
        "        for line in file:\n",
        "            try:\n",
        "                # Convert the line to an integer and append to the list\n",
        "                job_id = int(line.strip())\n",
        "                desired_levels.append(job_id)\n",
        "            except ValueError:\n",
        "                # Ignore lines that cannot be converted to integers\n",
        "                continue\n",
        "\n",
        "    print(f\"Job IDs loaded into desired_levels: {desired_levels}\")\n",
        "    return desired_levels"
      ],
      "metadata": {
        "id": "1JZKNxlox_vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder path containing the CSV files\n",
        "folder_path = \"path/to/the/Alibaba/Data\"\n",
        "\n",
        "# Specify the names of your CSV files\n",
        "file1_name = \"batch_task.csv\"\n",
        "file2_name = \"batch_instance.csv\"\n",
        "\n",
        "# Load each file into a separate DataFrame variable\n",
        "file1_path = os.path.join(folder_path, file1_name)\n",
        "file2_path = os.path.join(folder_path, file2_name)\n",
        "selected_columns = [\"instance_name\", \"task_name\", \"job_name\", \"start_time\", \"end_time\"]\n"
      ],
      "metadata": {
        "id": "Z-qr5bZSyIDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(file1_path, header=None, usecols=[0, 1, 2, 5, 6])\n",
        "df1 = df1[~df1[0].str.contains(r'^task_|MergeTask', na=False)]\n",
        "# Filter rows where column [2] matches job IDs in 'desired_levels'\n",
        "df1 = df1[df1[2].isin([f\"j_{job_id}\" for job_id in desired_levels])]\n",
        "# df1.head()"
      ],
      "metadata": {
        "id": "4tFeIyJGyJ-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define a function to extract the job ID (e.g., 'j_x' from \"j_123\")\n",
        "def extract_job_id(group_info):\n",
        "    match = re.match(r'j_(\\d+)', group_info)\n",
        "    return int(match.group(1)) if match else None"
      ],
      "metadata": {
        "id": "cTYw0-y-Mi1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to extract job IDs and add a column for job grouping\n",
        "df1['job_id'] = df1[2].apply(extract_job_id)\n",
        "print('Done with extract job_id')\n",
        "\n",
        "# Apply the function to create task dependencies\n",
        "df1['task_id'] = df1[0].str.extract(r'^[^\\d]*(\\d+)').astype(int)\n",
        "\n",
        "# Extract dependencies by finding all numbers after the first underscore and converting them to lists\n",
        "df1['dependencies'] = df1[0].str.findall(r'_(\\d+)').apply(lambda x: [int(dep) for dep in x])"
      ],
      "metadata": {
        "id": "xP78xuCzMks0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94cba882-cabf-4b07-f53b-993d9bab97f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done with extract job_id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create a grouped dictionary for further analysis\n",
        "# Initialize the grouped dictionary\n",
        "grouped_data = defaultdict(lambda: defaultdict(list))\n",
        "\n",
        "# Populate the dictionary and calculate duration in a single iteration\n",
        "for _, row in df1.iterrows():\n",
        "    job_id = row['job_id']\n",
        "    task_id = row['task_id']\n",
        "    # Simplified duration calculation\n",
        "    duration = row[6] - row[5]\n",
        "    # Append the row data along with calculated duration to grouped_data\n",
        "    task_info = row.values.tolist() + [duration]\n",
        "    grouped_data[job_id][task_id].append(task_info)\n",
        "\n",
        "print(\"Data grouped with duration successfully.\")"
      ],
      "metadata": {
        "id": "gz7QSYD6Mmrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db041d29-3407-41fb-fce1-f44ab72dc9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data grouped with duration successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group tasks by job ID and create the DAGs\n",
        "job_dags = {}\n",
        "hierarchy_levels = {}\n",
        "for job_id, group in tqdm(df1.groupby('job_id'), desc=\"Processing jobs\"):\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    # Add nodes and edges based on dependencies within each job\n",
        "    for _, row in group.iterrows():\n",
        "        task_id = row['task_id']\n",
        "        G.add_node(task_id)  # Add task as a node in the graph\n",
        "        for dep in row['dependencies']:\n",
        "            G.add_edge(dep, task_id)  # Add directed edge for each dependency\n",
        "\n",
        "    # Store the graph for this job\n",
        "    job_dags[job_id] = G\n",
        "\n",
        "    # Uncomment for drawing\n",
        "    try:\n",
        "        # Check if the graph is acyclic (i.e., no cycles)\n",
        "        if nx.is_directed_acyclic_graph(G):\n",
        "            hierarchy_levels[job_id] = nx.dag_longest_path_length(G)\n",
        "        else:\n",
        "            print(f\"Cycle detected in job {job_id}. Skipping longest path calculation.\")\n",
        "            hierarchy_levels[job_id] = None  # Or handle it as needed (e.g., set to 0 or other value)\n",
        "    except nx.NetworkXError as e:\n",
        "        print(f\"Error processing job {job_id}: {e}\")\n",
        "        hierarchy_levels[job_id] = 0  # Handle the error by assigning a default value or skipping the job\n",
        "\n",
        "# Filter out jobs with None values in hierarchy_levels\n",
        "filtered_hierarchy_levels = {job_id: level for job_id, level in hierarchy_levels.items() if level is not None}"
      ],
      "metadata": {
        "id": "9uv2KbYTMoxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder named \"levels\" if it doesn't already exist\n",
        "os.makedirs(\"levels\", exist_ok=True)\n",
        "\n",
        "# Loop through the unique values in the dictionary\n",
        "for value in set(filtered_hierarchy_levels.values()):\n",
        "    # Collect keys with the current value\n",
        "    desired_levels = [key for key, val in filtered_hierarchy_levels.items() if val == value]\n",
        "\n",
        "    # Create a file named \"levels_<value>.txt\" inside the \"levels\" folder\n",
        "    filename = os.path.join(\"levels\", f\"levels_{value}.txt\")\n",
        "    with open(filename, \"w\") as file:\n",
        "        # Write each key to the file\n",
        "        for level in desired_levels:\n",
        "            file.write(f\"{level}\\n\")\n",
        "\n",
        "    print(f\"Keys with value {value} saved to {filename}.\")"
      ],
      "metadata": {
        "id": "ND-c8mor2l4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load job IDs into desired_levels. Take DAG with 11 levels as an example\n",
        "level_file = \"levels/levels_11.txt\"\n",
        "desired_levels = load_job_ids(level_file)"
      ],
      "metadata": {
        "id": "nZtzSjzu2mYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the top DAGs from large csv file and\n",
        "# Numbers to match in the 'j_xxxxx' format\n",
        "# numbers_to_match = ['1161482', '1667543', '3105001']\n",
        "numbers_to_match = desired_levels\n",
        "pattern = '|'.join([f'j_{num}' for num in numbers_to_match])\n",
        "\n",
        "# Dictionary to count the number of rows collected for each number\n",
        "collected_rows = {num: [] for num in numbers_to_match}\n",
        "\n",
        "# Read the CSV file in chunks\n",
        "chunksize = 10**6  # Adjust chunk size if needed\n",
        "for chunk in pd.read_csv(file2_path, chunksize=chunksize, header=None):\n",
        "    # Filter rows that match the pattern in column 2\n",
        "    matching_rows = chunk[chunk[2].str.contains(pattern, na=False)]\n",
        "\n",
        "    # Use a single loop to categorize rows into collected_rows\n",
        "    matching_rows = matching_rows.copy()\n",
        "    matching_rows['group'] = matching_rows[2].str.extract(r'j_(\\d+)')[0].astype(int)  # Extract the number after \"j_\"\n",
        "    matching_rows = matching_rows[matching_rows['group'].isin(numbers_to_match)]  # Keep only relevant numbers\n",
        "\n",
        "    for number in numbers_to_match:\n",
        "        # Append rows matching the current number\n",
        "        rows = matching_rows[matching_rows['group'] == number]\n",
        "        collected_rows[number].extend(rows[[0, 1, 2, 5, 6, 10, 12]].values.tolist())\n",
        "\n",
        "    # Check if we have collected rows for all specified numbers\n",
        "    if all(collected_rows[num] for num in numbers_to_match):\n",
        "        break\n",
        "\n",
        "\n",
        "# Combine all collected rows into a single DataFrame\n",
        "filtered_rows = []\n",
        "for rows in collected_rows.values():\n",
        "    filtered_rows.extend(rows)\n",
        "df2 = pd.DataFrame(filtered_rows)\n",
        "df2.to_csv('instance_level.csv')\n",
        "\n",
        "\n",
        "# Load in df2 from local .csv file\n",
        "df2 = pd.read_csv('instance_level.csv',usecols=range(1,8))\n",
        "df2.columns=[0,1,2,3,4,5,6]\n",
        "df2['task_id'] = df2[1].str.extract(r'^[^\\d]*(\\d+)').astype(int)\n",
        "df2['instance_time'] = df2[4] - df2[3]  # Column 6 - Column 5\n",
        "df2['resource_usage_1'] = df2[5] * df2['instance_time']\n",
        "df2['resource_usage_2'] = df2[6] * df2['instance_time']\n",
        "# df2.head()"
      ],
      "metadata": {
        "id": "it9zGf877vGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find bottleneck for all of the selected DAGs\n",
        "# specified_job_ids = [1161482, 1667543, 3105001]  # Replace with the actual job IDs you want to analyze\n",
        "specified_job_ids = desired_levels\n",
        "# Dictionary to store selected DAGs and bottleneck points for the specified jobs\n",
        "selected_dags = {}\n",
        "bottleneck_points = {}\n",
        "\n",
        "# Process only the specified job IDs\n",
        "for job_id, group in tqdm(df1.groupby('job_id'), desc=\"Processing specified jobs\"):\n",
        "    if job_id in specified_job_ids:\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        # Add nodes and edges based on dependencies within each specified job\n",
        "        for _, row in group.iterrows():\n",
        "            task_id = row['task_id']\n",
        "            G.add_node(task_id)  # Add task as a node in the graph\n",
        "            for dep in row['dependencies']:\n",
        "                G.add_edge(dep, task_id)  # Add directed edge for each dependency\n",
        "\n",
        "        # Store the graph for this job\n",
        "        selected_dags[job_id] = G\n",
        "\n",
        "        # Step 1: Assign levels to nodes using topological sort\n",
        "        levels = defaultdict(list)\n",
        "        for node in nx.topological_sort(G):\n",
        "            level = 0\n",
        "            for pred in G.predecessors(node):\n",
        "                level = max(level, levels[pred][0] + 1)\n",
        "            levels[node] = [level]\n",
        "\n",
        "        # Step 2: Group nodes by levels\n",
        "        level_nodes = defaultdict(list)\n",
        "        for node, level_info in levels.items():\n",
        "            level_nodes[level_info[0]].append(node)\n",
        "\n",
        "        # Step 3: Find levels with only one node, excluding source and sink levels\n",
        "        unique_levels = [\n",
        "            level for level, nodes in level_nodes.items()\n",
        "            if len(nodes) == 1 and level != 0 and level != max(level_nodes.keys())\n",
        "        ]\n",
        "\n",
        "        unique_nodes=[\n",
        "            key for key, val_list in levels.items() if any(target in val_list for target in unique_levels)\n",
        "        ]\n",
        "\n",
        "        # Step 3: Find the bottleneck node based on the conditions\n",
        "        for node in unique_nodes:\n",
        "            # Get ancestors, descendants, and check the condition\n",
        "            ancestors = set(nx.ancestors(G, node))\n",
        "            descendants = set(nx.descendants(G, node))\n",
        "            remaining_nodes = set(G.nodes) - ancestors - descendants - {node}\n",
        "\n",
        "            # Ensure remaining nodes is empty and levels above/below have more than one node\n",
        "            node_level = levels[node][0]\n",
        "            above_level_nodes = [n for n in levels if levels[n][0] == node_level - 1]\n",
        "            below_level_nodes = [n for n in levels if levels[n][0] == node_level + 1]\n",
        "\n",
        "            # Count in-degree and out-degree\n",
        "            in_degree = G.in_degree(node)  # Number of edges pointing to the node\n",
        "            out_degree = G.out_degree(node)  # Number of edges pointing out from the node\n",
        "\n",
        "            # print('checking',node, remaining_nodes, in_degree, out_degree)\n",
        "\n",
        "            if len(remaining_nodes) == 0 and (in_degree > 1 or out_degree > 1):\n",
        "                bottleneck_points[job_id] = node\n",
        "\n",
        "# Remove job_id entries from selected_dags that are not in bottleneck_points\n",
        "selected_dags = {job_id: G for job_id, G in selected_dags.items() if job_id in bottleneck_points}"
      ],
      "metadata": {
        "id": "qGbWxv2v9D_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Three calculate at once\n",
        "# Resource capacities (replace with actual values)\n",
        "resource_capacity_1 = 9600\n",
        "resource_capacity_2 = 100\n",
        "\n",
        "# Dictionaries to store the critical path lengths, twork, and modcp for each sub-DAG\n",
        "cplength = defaultdict(lambda: {\"subdag_1\": 0, \"subdag_2\": 0})\n",
        "twork_subdags = defaultdict(lambda: {\"subdag_1\": 0, \"subdag_2\": 0})\n",
        "modcp = defaultdict(lambda: {\"subdag_1\": 0, \"subdag_2\": 0})\n",
        "\n",
        "# Iterate over each DAG\n",
        "for job_id, G in selected_dags.items():\n",
        "    # Get the bottleneck node for this DAG\n",
        "    bottleneck_node = bottleneck_points[job_id]\n",
        "    subdag_1_nodes = set(nx.ancestors(G, bottleneck_node))\n",
        "    subdag_2_nodes = set(nx.descendants(G, bottleneck_node)).union({bottleneck_node})\n",
        "\n",
        "    # Create two sub-DAGs\n",
        "    subdag_1 = G.subgraph(subdag_1_nodes).copy()\n",
        "    subdag_2 = G.subgraph(subdag_2_nodes).copy()\n",
        "\n",
        "    # Step 1: Calculate critical path length for both sub-DAGs\n",
        "    task_durations = {task_id: sum(row[-1] for row in grouped_data[job_id][task_id]) for task_id in G.nodes}\n",
        "\n",
        "    def calculate_longest_path_duration(subdag, task_durations):\n",
        "        max_path_duration = 0\n",
        "        start_nodes = [node for node in subdag.nodes if subdag.in_degree(node) == 0]\n",
        "        end_nodes = [node for node in subdag.nodes if subdag.out_degree(node) == 0]\n",
        "\n",
        "        # Calculate longest path for each path in the sub-DAG\n",
        "        for start_node in start_nodes:\n",
        "            for end_node in end_nodes:\n",
        "                for path in nx.all_simple_paths(subdag, source=start_node, target=end_node):\n",
        "                    path_duration = sum(task_durations[task] for task in path)\n",
        "                    max_path_duration = max(max_path_duration, path_duration)\n",
        "        return max_path_duration\n",
        "\n",
        "    cplength[job_id][\"subdag_1\"] = calculate_longest_path_duration(subdag_1, task_durations)\n",
        "    cplength[job_id][\"subdag_2\"] = calculate_longest_path_duration(subdag_2, task_durations)\n",
        "\n",
        "    # Step 2: Calculate `twork` for both sub-DAGs\n",
        "    for subdag_name, subdag_nodes in zip([\"subdag_1\", \"subdag_2\"], [subdag_1_nodes, subdag_2_nodes]):\n",
        "        subdag_group = df2[(df2[2] == f\"j_{job_id}\") & (df2['task_id'].isin(subdag_nodes))]\n",
        "\n",
        "        total_usage_1 = subdag_group['resource_usage_1'].sum()\n",
        "        total_usage_2 = subdag_group['resource_usage_2'].sum()\n",
        "\n",
        "        max_usage = max(total_usage_1 / resource_capacity_1, total_usage_2 / resource_capacity_2)\n",
        "        twork_subdags[job_id][subdag_name] = max_usage\n",
        "\n",
        "    # Step 3: Calculate `modcp` for each sub-DAG\n",
        "    for subdag_name, subdag_nodes in zip([\"subdag_1\", \"subdag_2\"], [subdag_1_nodes, subdag_2_nodes]):\n",
        "        subdag = G.subgraph(subdag_nodes).copy()  # Extract the sub-DAG graph\n",
        "\n",
        "        # Calculate `cplength` and `twork` for each task\n",
        "        task_metrics = {}\n",
        "        for task_id in subdag.nodes:\n",
        "            matching_rows = df2[(df2[2] == f\"j_{job_id}\") & (df2['task_id'] == task_id)]\n",
        "\n",
        "            # Calculate task-specific cplength\n",
        "            task_cplength = matching_rows[\"instance_time\"].sum()\n",
        "\n",
        "            # Calculate task-specific twork\n",
        "            resource_usage_1_sum = matching_rows[\"resource_usage_1\"].sum()\n",
        "            resource_usage_2_sum = matching_rows[\"resource_usage_2\"].sum()\n",
        "            task_twork = max(resource_usage_1_sum / resource_capacity_1, resource_usage_2_sum / resource_capacity_2)\n",
        "\n",
        "            task_metrics[task_id] = {\"cplength\": task_cplength, \"twork\": task_twork}\n",
        "\n",
        "        # Retrieve minimum instance time for each task in the sub-DAG from grouped_data\n",
        "        min_instance_times = {task_id: min(row[-1] for row in grouped_data[job_id][task_id]) for task_id in subdag.nodes}\n",
        "\n",
        "        # Calculate modcp for each path in the sub-DAG\n",
        "        max_modcp_path = 0\n",
        "        start_nodes = [node for node in subdag.nodes if subdag.in_degree(node) == 0]\n",
        "        end_nodes = [node for node in subdag.nodes if subdag.out_degree(node) == 0]\n",
        "\n",
        "        for start_node in start_nodes:\n",
        "            for end_node in end_nodes:\n",
        "                for path in nx.all_simple_paths(subdag, source=start_node, target=end_node):\n",
        "                    # For each path, calculate modcp\n",
        "                    path_modcp_values = []\n",
        "\n",
        "                    for i, task in enumerate(path):\n",
        "                        task_cplength = task_metrics[task][\"cplength\"]\n",
        "                        task_twork = task_metrics[task][\"twork\"]\n",
        "                        max_cplength_twork = max(task_cplength, task_twork)\n",
        "\n",
        "                        min_times_sum = sum(\n",
        "                            min_instance_times[other_task]\n",
        "                            for j, other_task in enumerate(path) if i != j\n",
        "                        )\n",
        "\n",
        "                        path_modcp_value = max_cplength_twork + min_times_sum\n",
        "                        path_modcp_values.append(path_modcp_value)\n",
        "\n",
        "                    path_max_modcp = max(path_modcp_values)\n",
        "                    max_modcp_path = max(max_modcp_path, path_max_modcp)\n",
        "\n",
        "        modcp[job_id][subdag_name] = max_modcp_path"
      ],
      "metadata": {
        "id": "N_lwBu_I8QH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print information within modcp, cplength, and twork_subdags for testing\n",
        "print(\"=== modcp ===\")\n",
        "for job_id, subdags in modcp.items():\n",
        "    print(f\"Job ID: {job_id}\")\n",
        "    for subdag_name, modcp_value in subdags.items():\n",
        "        print(f\"  {subdag_name}: modcp = {modcp_value}\")\n",
        "    break\n",
        "\n",
        "print(\"=== cplength ===\")\n",
        "for job_id, subdags in cplength.items():\n",
        "    print(f\"Job ID: {job_id}\")\n",
        "    for subdag_name, cplength_value in subdags.items():\n",
        "        print(f\"  {subdag_name}: cplength = {cplength_value}\")\n",
        "    break\n",
        "\n",
        "print(\"=== twork_subdags ===\")\n",
        "for job_id, subdags in twork_subdags.items():\n",
        "    print(f\"Job ID: {job_id}\")\n",
        "    for subdag_name, twork_value in subdags.items():\n",
        "        print(f\"  {subdag_name}: twork = {twork_value}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "JPETAs3m8UOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionaries to store the newlb, summed cplength, and twork for each DAG\n",
        "newlb = {}\n",
        "summed_cplength = {}\n",
        "summed_twork = {}\n",
        "\n",
        "# Iterate over each job (DAG) to calculate the newlb, summed cplength, and summed twork\n",
        "for job_id in selected_dags.keys():\n",
        "    # Calculate the maximum value between cplength, twork, and modcp for each sub-DAG\n",
        "    subdag_1_max = max(cplength[job_id][\"subdag_1\"], twork_subdags[job_id][\"subdag_1\"], modcp[job_id][\"subdag_1\"])\n",
        "    subdag_2_max = max(cplength[job_id][\"subdag_2\"], twork_subdags[job_id][\"subdag_2\"], modcp[job_id][\"subdag_2\"])\n",
        "\n",
        "    # Sum the maximum values of the two sub-DAGs for newlb\n",
        "    newlb_value = subdag_1_max + subdag_2_max\n",
        "    newlb[job_id] = newlb_value\n",
        "\n",
        "    # Sum the cplength values for both sub-DAGs\n",
        "    summed_cplength_value = cplength[job_id][\"subdag_1\"] + cplength[job_id][\"subdag_2\"]\n",
        "    summed_cplength[job_id] = summed_cplength_value\n",
        "\n",
        "    # Sum the twork values for both sub-DAGs\n",
        "    summed_twork_value = twork_subdags[job_id][\"subdag_1\"] + twork_subdags[job_id][\"subdag_2\"]\n",
        "    summed_twork[job_id] = summed_twork_value\n",
        "\n",
        "# Print the results\n",
        "print(\"=== Results ===\")\n",
        "for job_id in selected_dags.keys():\n",
        "    print(f\"Job ID: {job_id}\")\n",
        "    print(f\"  newlb = {newlb[job_id]}\")\n",
        "    print(f\"  Summed cplength = {summed_cplength[job_id]}\")\n",
        "    print(f\"  Summed twork = {summed_twork[job_id]}\")\n",
        "    break\n",
        "\n"
      ],
      "metadata": {
        "id": "m8qQlp6R8h4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File path for saving the results\n",
        "output_file = \"level_graphene_result.txt\"\n",
        "\n",
        "# Save data to the file\n",
        "with open(output_file, \"w\") as file:\n",
        "    # Write the header row\n",
        "    file.write(\"Job ID\\tNewLB\\tSummed CPLength\\tSummed TWork\\n\")\n",
        "\n",
        "    # Write data for each job_id\n",
        "    for job_id in selected_dags.keys():\n",
        "        file.write(f\"{job_id}\\t{newlb[job_id]}\\t{summed_cplength[job_id]}\\t{summed_twork[job_id]}\\n\")\n",
        "\n",
        "print(f\"Results saved to {output_file}\")\n",
        "\n",
        "# Aggregate resource utilization by job_id\n",
        "resource_utilization_by_job = defaultdict(lambda: {\"resource_1\": 0, \"resource_2\": 0})\n",
        "\n",
        "for job_id in selected_dags.keys():  # Iterate over the job_ids from selected_dags\n",
        "    # Filter df2 for the current job_id\n",
        "    job_df = df2[df2[2] == f\"j_{job_id}\"]\n",
        "\n",
        "    # Sum resource usage for the job\n",
        "    total_usage_1 = job_df[5].sum()\n",
        "    total_usage_2 = job_df[6].sum()\n",
        "\n",
        "    # Normalize by resource capacities and store the results\n",
        "    resource_utilization_by_job[job_id][\"resource_1\"] = total_usage_1 / resource_capacity_1\n",
        "    resource_utilization_by_job[job_id][\"resource_2\"] = total_usage_2 / resource_capacity_2\n",
        "\n",
        "# Save the normalized utilization into a .txt file\n",
        "output_file = \"resource_utilization_by_job.txt\"\n",
        "\n",
        "with open(output_file, \"w\") as file:\n",
        "    # Write header\n",
        "    file.write(\"Job ID\\tResource 1 Utilization\\tResource 2 Utilization\\n\")\n",
        "\n",
        "    # Write data\n",
        "    for job_id, utilization in resource_utilization_by_job.items():\n",
        "        file.write(f\"{job_id}\\t{utilization['resource_1']:.6f}\\t{utilization['resource_2']:.6f}\\n\")\n",
        "\n",
        "print(f\"Normalized resource utilization saved to {output_file}\")"
      ],
      "metadata": {
        "id": "HV1nn7mb8k5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}